#!/usr/bin/env python

# Copyright 2019 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: v1
data:
    sanity_checks.yaml: |
          manifest: |
            apiVersion: batch/v1
            kind: Job
            metadata:
              generateName: sanity-checks-{{inputs.parameters.name}}-
              namespace: metacontroller
            spec:
              template:
                metadata:
                  labels:
                    resiliency: enabled
                spec:
                  nodeSelector:
                    resiliency: enabled
                  affinity:
                    podAffinity:
                      requiredDuringSchedulingIgnoredDuringExecution:
                      - labelSelector:
                          matchExpressions:
                          - key: resiliency
                            operator: In
                            values:
                            - enabled
                        topologyKey: "kubernetes.io/hostname"
                  restartPolicy: OnFailure
                  containers:
                  - command: {{inputs.parameters.command}}
                    name: {{inputs.parameters.name}}
                    image: {{inputs.parameters.image}}
                    volumeMounts:
                      - name: tz-config
                        mountPath: /etc/localtime
                      - name: torpedo
                        mountPath: /var/log
                      - name: kubeconfig
                        mountPath: /root/.kube/config
                        subPath: config
                  volumes:
                  - name: tz-config
                    hostPath:
                       path: /etc/localtime
                  - name: torpedo
                    persistentVolumeClaim:
                      claimName: {{inputs.parameters.volume-name}}
                  - name: kubeconfig
                    configMap:
                      name: {{inputs.parameters.config-name}}

    chaos-parameters.yaml: |
          chaos-parameters:
          - target-namespace
          - pod-labels
          - node-labels
          - same-node
          - max-nodes
          - kill-count
          - kill-interval
          - job-duration
          - count
          - image
          - args
          - command
          - env
    traffic-parameters.yaml: |
          traffic-parameters:
          - target-namespace
          - auth
          - job-duration
          - count
          - nodes
          - traffic-module
          - extra-args
          - pod-labels
          - endpoint
          - request-type
          - extra-headers
          - body
    torpedo_argo.j2: |
          {% set orchestrator_dependency = [] %}
          {% set job=spec['job-params'] %}
          {% set chaos_dependency = [] %}
          {% set service_list = [] %}
          {% set torpedo_name = metadata['name'] %}
          {% set torpedo_namespace = metadata['namespace'] %}
          {% set traffic_name_list = [] %}
          {% set chaos_name_list = [] %}

          apiVersion: argoproj.io/v1alpha1
          kind: Workflow
          metadata:
            name: {{ torpedo_name }}
            namespace: metacontroller
          spec:
            nodeSelector:
              resiliency: enabled
            entrypoint: torpedo-dag
            serviceAccountName: resiliency
            templates:
            - name: torpedo-dag
              dag:
                tasks:
                - name: remote-cluster-kube-config
                  template: remote-cluster-kube-config
                - name: volume-pvc
                  template: volume-pvc
                {% if "traffic-params" in job %}
                - name: traffic-orchestrator
                  template: torpedo-job
                  arguments:
                    parameters:
                    - name: name
                      value: {{ torpedo_name }}
                    - name: config-name
                      {% if 'cluster-config' in job %}
                      value: '{{job['cluster-config']['name']}}'
                      {% else %}
                      value: remote-cluster-kube-config
                      {% endif %}
                    - name: nodes
                      value: '{{job['chaos-params']['nodes']}}'
                    - name: pod-labels
                      value: "{{job['chaos-params']['pod-labels']}}"
                    {% for key, value in job.items() %}
                    {% if key != "sanity-checks" and key != 'chaos-params' and key != 'traffic-params'%}
                    - name: {{ key }}
                      value: "{{ value }}"
                    {% endif %}
                    {% endfor %}
                    {% for key, value in job['traffic-params'].items() %}
                    - name: {{ key }}
                      value: "{{ value }}"
                    {% endfor %}
                    {% for param in spec['traffic_parameters']%}
                    {% if param not in job and param not in job['traffic-params'] and param not in job['chaos-params'] %}
                    - name: {{ param }}
                      value: ''
                    {% endif %}
                    {% endfor %}
                {{ traffic_name_list.append("traffic-orchestrator") }}
                {% endif %}
                {% if 'chaos-params' in job %}
                - name: chaos-orchestrator
                  template: chaos-job
                  arguments:
                    parameters:
                    - name: name
                      value: {{ torpedo_name }}
                    - name: config-name
                      {% if 'cluster-config' in job %}
                      value: '{{job['cluster-config']['name']}}'
                      {% else %}
                      value: remote-cluster-kube-config
                      {% endif %}
                    {% set added_params = [] %}
                    {% for key, value in job.items() %}
                    {% if key != "sanity-checks" and key != 'chaos-params' and key != 'traffic-params'%}
                    - name: {{ key }}
                      value: "{{ value }}"
                    {{ added_params.append(key) }}
                    {% endif %}
                    {% endfor %}
                    {% for key, value in job['chaos-params'].items() %}
                    - name: {{ key }}
                      value: "{{ value }}"
                    {{ added_params.append(key) }}
                    {% endfor %}
                    {% if 'plugin' in job['chaos-params'] %}
                    {% for key, value in job['chaos-params']['plugin'].items() %}
                    {{ added_params.append(key) }}
                    - name: {{ key }}
                      value: "{{ value }}"
                    {% endfor %}
                    {% endif %}
                    {% for param in spec['chaos_parameters']%}
                    {% if param not in added_params %}
                    - name: {{ param }}
                      value: ''
                    {% endif %}
                    {% endfor %}
                {{ chaos_name_list.append("chaos-orchestrator") }}
                {% endif %}
                {{ service_list.append({"service": torpedo_name , "duration": job['job-duration']}) }}
                {% if 'sanity-checks' in job %}
                - name: {{ "sanity-checks-" +  torpedo_name }}
                  {% if orchestrator_dependency %}
                  dependencies: {{orchestrator_dependency + chaos_dependency}}
                  {% endif %}
                  template: sanity-checks
                  arguments:
                    parameters:
                    - name: volume-name
                      value: {{job['volume-name']}}
                    - name: config-name
                      {% if 'cluster-config' in job %}
                      value: '{{job['cluster-config']['name']}}'
                      {% else %}
                      value: remote-cluster-kube-config
                      {% endif %}
                    - name: name
                      value: {{ torpedo_name }}
                    - name: namespace
                      value: {{ job['target-namespace'] }}
                    - name: command
                      value: |
                        {{job['sanity-params']['command']}}
                    - name: image
                      value: {{job['sanity-params']['image']}}
                  {% endif %}
                {{ orchestrator_dependency.extend(traffic_name_list) }}
                {{ chaos_dependency.extend(chaos_name_list) }}
                - name: log-analysis
                  {% if orchestrator_dependency or chaos_dependency %}
                  dependencies: {{orchestrator_dependency + chaos_dependency}}
                  {% endif %}
                  template: log-analysis

            - name: remote-cluster-kube-config
              resource:
                action: create
                manifest: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    {% if 'cluster-config' in job %}
                    name: "{{job['cluster-config']['name']}}"
                    {% else %}
                    name: remote-cluster-kube-config
                    {% endif %}
                    namespace: metacontroller
                  data:
                    config: |
                          apiVersion: v1
                          kind: Config
                          clusters:
                          - name: default-cluster
                            cluster:
                              insecure-skip-tls-verify: true
                              {% if 'cluster-config' in job %}
                              server: "{{job['cluster-config']['apiserver']}}"
                              {% else %}
                              server: 'https://kubernetes.default.svc.cluster.local:443'
                              {% endif %}
                          contexts:
                          - name: default-context
                            context:
                              cluster: default-cluster
                              namespace: default
                              user: default-user
                          current-context: default-context
                          users:
                          - name: default-user
                            user:
                              {% if 'cluster-config' in job %}
                              token: "{{job['cluster-config']['token']}}"
                              {% else %}
                              token: "{{spec['remote-cluster-token']}}"
                              {% endif %}
            - name: volume-pvc
              resource:
                action: create
                manifest: |
                  kind: PersistentVolumeClaim
                  apiVersion: v1
                  metadata:
                    name: "{{job['volume-name']}}"
                    namespace: metacontroller
                  spec:
                    accessModes: [ "ReadWriteOnce" ]
                    resources:
                      requests:
                        storage: "{{job['volume-storage']}}"
                    storageClassName: "{{job['volume-storage-class']}}"

            - name: sanity-checks
              inputs:
                parameters:
                - name: command
                - name: name
                - name: image
                - name: volume-name
                - name: config-name

              resource:
                action: create
                successCondition: status.succeeded > 0
                failureCondition: status.failed > 3
                manifest: |
                  {{spec['torpedo_sanity_checks'] |indent(8)}}
            - name: torpedo-job
              inputs:
                parameters:
                {% for param in spec['traffic_parameters'] %}
                - name: {{ param }}
                {% endfor %}
                - name: name
                - name: config-name
                - name: volume-name
              resource:
                action: create
                successCondition: status.succeeded > 0
                failureCondition: status.failed > 3
                manifest: |
                  {{spec['torpedo_traffic_job_manifest']|indent(8)}}

            - name: chaos-job
              inputs:
                parameters:
                {% for param in spec['chaos_parameters'] %}
                - name: {{ param }}
                {% endfor %}
                - name: name
                - name: config-name
                - name: volume-name
              nodeSelector:
                resiliency: enabled
              resource:
                action: create
                successCondition: status.succeeded > 0
                failureCondition: status.failed > 3
                manifest: |
                {% if 'plugin' in job['chaos-params'] %}
                  {{spec['torpedo_chaos_plugin_job_manifest']|indent(8) }}
                {% else %}
                  {{spec['torpedo_chaos_job_manifest']|indent(8) }}
                {% endif %}

            - name: log-analysis
              resource:
                action: create
                successCondition: status.succeeded > 0
                failureCondition: status.failed > 3
                manifest: |
                  apiVersion: batch/v1
                  kind: Job
                  metadata:
                    generateName: {{ torpedo_name }}-log-analyzer-
                    namespace: metacontroller
                  spec:
                    template:
                      metadata:
                        labels:
                          log-collector: enabled
                      spec:
                        serviceAccountName: resiliency
                        nodeSelector:
                          log-collector: enabled
                        restartPolicy: OnFailure
                        containers:
                        - command:
                          - python3
                          - /opt/torpedo/log_analyzer.py
                          - "{{service_list}}"
                          name: log-collector
                          image: gpsingh1991/torpedo-chaos-plugin:v1
                          volumeMounts:
                            - name: torpedo
                              mountPath: /var/log
                            - name: tz-config
                              mountPath: /etc/localtime

                        volumes:
                        - name: tz-config
                          hostPath:
                             path: /etc/localtime
                        - name: torpedo
                          persistentVolumeClaim:
                            claimName: {{job['volume-name']}}

    torpedo-chaos-job.yaml: |
        manifest: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: {{inputs.parameters.name}}-{{inputs.parameters.target-namespace}}-resilliency-chaos-test
            namespace: metacontroller
          spec:
            template:
              metadata:
                labels:
                  resiliency: enabled
              spec:
                serviceAccountName: resiliency
                nodeSelector:
                  resiliency: enabled
                affinity:
                  podAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                    - labelSelector:
                        matchExpressions:
                        - key: resiliency
                          operator: In
                          values:
                          - enabled
                      topologyKey: "kubernetes.io/hostname"
                restartPolicy: OnFailure
                containers:
                - command:
                  - python3
                  - /opt/torpedo/kube_chaos.py
                  - "{{inputs.parameters.target-namespace}}"
                  - "{{inputs.parameters.job-duration}}"
                  - "{{inputs.parameters.count}}"
                  - "{{inputs.parameters.kill-interval}}"
                  - "{{inputs.parameters.pod-labels}}"
                  - "{{inputs.parameters.node-labels}}"
                  - "{{inputs.parameters.same-node}}"
                  - "{{inputs.parameters.kill-count}}"
                  - "{{inputs.parameters.max-nodes}}"
                  name: {{inputs.parameters.name}}
                  image: gpsingh1991/torpedo-chaos-plugin:v1
                  volumeMounts:
                    - name: tz-config
                      mountPath: /etc/localtime
                    - name: torpedo
                      mountPath: /var/log
                    - name: kubeconfig
                      mountPath: /root/.kube/config
                      subPath: config
                volumes:
                - name: tz-config
                  hostPath:
                     path: /etc/localtime
                - name: torpedo
                  persistentVolumeClaim:
                    claimName: {{inputs.parameters.volume-name}}
                - name: kubeconfig
                  configMap:
                    name: {{inputs.parameters.config-name}}

    torpedo-chaos-plugin-job.yaml: |
        manifest: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: {{inputs.parameters.name}}-{{inputs.parameters.target-namespace}}-resilliency-chaos-test
            namespace: metacontroller
          spec:
            template:
              metadata:
                labels:
                  resiliency: enabled
              spec:
                serviceAccountName: resiliency
                nodeSelector:
                  resiliency: enabled
                affinity:
                  podAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                    - labelSelector:
                        matchExpressions:
                        - key: resiliency
                          operator: In
                          values:
                          - enabled
                      topologyKey: "kubernetes.io/hostname"
                restartPolicy: OnFailure
                containers:
                - command: {{inputs.parameters.command}}
                  name: {{inputs.parameters.name}}
                  image: {{inputs.parameters.image}}
                  args: {{inputs.parameters.args}}
                  env: {{inputs.parameters.env}}
                  volumeMounts:
                    - name: tz-config
                      mountPath: /etc/localtime
                    - name: torpedo
                      mountPath: /var/log
                    - name: kubeconfig
                      mountPath: /root/.kube/config
                      subPath: config
                volumes:
                - name: tz-config
                  hostPath:
                     path: /etc/localtime
                - name: torpedo
                  persistentVolumeClaim:
                    claimName: {{inputs.parameters.volume-name}}
                - name: kubeconfig
                  configMap:
                    name: {{inputs.parameters.config-name}}
    torpedo-traffic-orchestrator.yaml: |
        manifest: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: {{inputs.parameters.name}}-{{inputs.parameters.target-namespace}}-resilliency-traffic-test
            namespace: metacontroller
          spec:
            template:
              metadata:
                labels:
                  resiliency: enabled
              spec:
                serviceAccountName: resiliency
                nodeSelector:
                  resiliency: enabled
                affinity:
                  podAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                    - labelSelector:
                        matchExpressions:
                        - key: resiliency
                          operator: In
                          values:
                          - enabled
                      topologyKey: "kubernetes.io/hostname"
                restartPolicy: OnFailure
                containers:
                - command:
                  - python3
                  - /opt/torpedo/main.py
                  - "{{inputs.parameters.auth}}"
                  - {{inputs.parameters.traffic-module}}
                  - "{{inputs.parameters.job-duration}}"
                  - "{{inputs.parameters.count}}"
                  - "{{inputs.parameters.nodes}}"
                  - "{{inputs.parameters.extra-args}}"
                  - "{{inputs.parameters.pod-labels}}"
                  - "{{inputs.parameters.endpoint}}"
                  - "{{inputs.parameters.request-type}}"
                  - "{{inputs.parameters.extra-headers}}"
                  - "{{inputs.parameters.body}}"
                  name: {{inputs.parameters.name}}
                  image: gpsingh1991/torpedo-traffic-generator:v3
                  volumeMounts:
                    - name: tz-config
                      mountPath: /etc/localtime
                    - name: torpedo
                      mountPath: /var/log
                    - name: kubeconfig
                      mountPath: /root/.kube/config
                      subPath: config
                volumes:
                - name: tz-config
                  hostPath:
                     path: /etc/localtime
                - name: torpedo
                  persistentVolumeClaim:
                    claimName: {{inputs.parameters.volume-name}}
                - name: kubeconfig
                  configMap:
                    name: {{inputs.parameters.config-name}}



kind: ConfigMap
metadata:
  name: torpedo-controller
  namespace: metacontroller
  labels:
    app: resiliency
